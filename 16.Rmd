---
title: "Chapter 16: Design and sample size decisions"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

```{r, eval = F, echo = F}
https://github.com/avehtari/ROS-Examples/
```

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# Design and sample size decisions

> In the present chapter, we consider the design of studies, in particular asking the question of what sample size is required to estimate a quantity of interest to some desired precision. We focus on the paradigmatic inferential tasks of estimating population averages, proportions, and comparisons in sample surveys, or estimating treatment effects in experiments and observational studies... As we frame it, the goal of design is not to attain statistical significance with some high probability, but rather to have a sense--before and after data have been collected--about what can realistically be learned from statistical analysis of an empirical study. (p. 291)

## 16.1 The problem with statistical power

> Statistical *power* is defined as the probability, before a study is performed, that a particular comparison will achieve "statistical significance" at some predetermined level (typically a $p$-value below 0.05), given some assumed true effect size. A power analysis is performed by first hypothesizing an effect size, then making some assumptions about the variation in the data and the sample size of the study to be conducted, and finally using probability calculations to determine the chance of the $p$-value being below the threshold.
>
> The conventional view is that you should avoid low-power studies because they are unlikely to succeed. (p. 291, *emphasis* in the original)

### 16.1.1 The winner's curse in low-power studies.

> The problem with the conventional reasoning is that, in a low-power study, the seeming "win" of statistical significance can actually be a trap. Economists speak of a "winner's curse" in which the highest bidder in an auction will, on average, be overpaying. Research studies--even randomized experiments--suffer from a similar winner's curse, that by focusing on comparisons that are statistically significant, we (the scholarly community as well as individual researchers) get a systematically biased and over-optimistic picture of the world.
>
> Put simply, when signal is low and noise is high, statistically significant patterns in data are likely to be wrong, in the sense that the results are unlikely to replicate.
>
> To put it in technical terms, statistically significant results are subject to type M and type S errors. (p. 292)

Here's how to make Figure 16.1.

```{r, warning = F, message = F, fig.width = 7, fig.height = 3.25}
library(tidyverse)

# set the global plotting theme
theme_set(theme_linedraw() +
            theme(panel.grid = element_blank()))

# set parameters
true_effect <- 2
se <- 8.1

# simulate
tibble(x = seq(from = -31, to = 36, by = 0.1)) %>% 
  mutate(d = dnorm(x, true_effect, sd = se)) %>% 
  mutate(fill = ifelse(x < se * -1.96, "a", 
                       ifelse(x > se * 1.96, "c", "b"))) %>% 
  
  # plot!
  ggplot(aes(x = x, y = d)) +
  geom_area(aes(fill = fill)) +
  geom_linerange(data = . %>% filter(x %in% c(0, true_effect)),
                 aes(ymin = 0, ymax = d, 
                     size = x > 0, linetype = x > 0)) +
  geom_line() +
  annotate(geom = "text",
           x = c(-31, 2.5, 22, -28), y = c(0.002, 0.008, 0.004, 0.04),
           label = c("You think you won.\nBut you lost.\nYour estimate has\nthe wrong sign!",
                     "True\neffect size\n(assumed\nto be 2% in\nthis example)",
                     "You think you won.\nBut you lost.\nYour estimate is\nover 8 times too high!",
                     "Because of the statistical significance filter,\nyour paper is not getting published if\nyour estimate is in the white zone."),
           hjust = 0, vjust = 0, size = 3.25) +
  scale_fill_manual(values = c(alpha("blue3", 1/2), "white", alpha("red3", 1/2))) +
  scale_size_manual(values = c(1/4, 1)) +
  scale_linetype_manual(values = c(2, 1)) +
  scale_x_continuous("Estimated effect size", breaks = -3:3 * 10,
                     labels = function(x) str_c(x, "%")) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = "The winner's curse of the low−power study") +
  theme(plot.subtitle = element_text(hjust = 0.5),
        legend.position = "none")
```

"We can examine the statistical properties of the estimate using the normal distribution: conditional on it being statistically significant (that is, at least two standard errors from zero), the estimate has at least a 24% probability of being in the wrong direction" (p. 292). The `pnorm()` will come in handy, for that.

```{r}
# probability of being negative, given the statistical significance filter
(p_negative <- pnorm(-1.96 * se, mean = true_effect, sd = se))

# probability of being positive, given the statistical significance filter
(p_positive <- 1 - pnorm(1.96 * se, mean = true_effect, sd = se))

# probability of being the wrong sign if published
p_negative / (p_negative + p_positive)
```

Further, your estimate "is, by necessity, over 8 times larger than the true effect" (p. 292). Here's how to compute that.

```{r}
(1.96 * se) / true_effect
```

"A study with these characteristics has essentially no chance of providing useful information, and we can say this even before the data have been collected. Given the numbers above for standard error and possible effect size, the study has a power of at most 6%" (p. 292). Since we already have `p_negative` and `p_positive`, this is just addition.

```{r}
p_negative + p_positive
```

"Thus, a key risk for a low-power study is not so much that it has a small chance of succeeding, but rather that an apparent success merely masks a larger failure" (p. 292).

### 16.1.2 Hypothesizing an effect size.

Some strategies for choosing an effect size for a power analysis are:

* entertaining a range of values consistent with the literature, or
* using the smallest effect size of interest.

## 16.2 General principles of design, as illustrated by estimates of proportions

### 16.2.1 Effect sizes and sample sizes.

"In designing a study, it is generally better, if possible, to double the effect size $\theta$ than to double the sample size $n$, since standard errors of estimation decrease with the square root of the sample size" (p. 293).

### 16.2.2 Published results tend to be overestimates.

"There are various reasons why we would typically expect future effects to be smaller than published estimates" (p. 293).

### 16.2.3 Design calculations.

"Before data are collected, it can be useful to estimate the precision of inferences that one expects to achieve with a given sample size, or to estimate the sample size required to attain a certain precision" (p. 294). This can be done with the conventional statistical-significance framework, but can also be operationalized in terms of the size of the standard error or the width of the 95% intervals.

### 16.2.4 Sample size to achieve a specified standard error.

Recall that we can compute the standard error of a proportion by

$$se = \sqrt{p(1 - p) / n},$$

where $p$ is the proportion and $n$ the sample size. If you work out the algebra, we can then solve for the required $n$ given $p$ and a $se$ threshold by

$$n = \left( \frac{\sqrt{p(1 - p)}}{se} \right)^2$$

We might make that into a custom function called `n_given_p_and_se()`.

```{r}
n_given_p_and_se <- function(p, se) {
  (sqrt(p * (1 - p)) / se)^2
}
```

Here are the minimum sample sizes for $se \leq 0.05$ and $p$ of $.5$ and $.6$.

```{r}
n_given_p_and_se(p = .5, se = 0.05)
n_given_p_and_se(p = .6, se = 0.05)
```

### 16.2.5 Sample size to achieve a specified probability of obtaining statistical significance.

Here's how we might compute the 95% CI's under the assumption $p = .6$, using

$$se = \sqrt{p(1 - p) / n}.$$

```{r, fig.width = 4, fig.height = 2.75}
p <- .6

tibble(n = 6:100) %>% 
  mutate(se = sqrt(p * (1 - p) / n)) %>% 
  mutate(ll = p + -1.96 * se,
         ul = p + +1.96 * se) %>% 
  mutate(powered = ifelse(ll > .5, TRUE, FALSE)) %>% 
  
  ggplot(aes(x = n, ymin = ll, ymax = ul, group = n)) + 
  geom_hline(yintercept = .5, linetype = 2, size = 1/4) +
  geom_linerange(aes(color = powered == TRUE)) +
  scale_color_viridis_d(option = "A", end = .6, breaks = NULL) +
  scale_x_continuous(expression(italic(n)), breaks = 0:4 * 25, 
                     expand = c(0, 0), limits = c(0, 100)) +
  scale_y_continuous(expression(italic(p)),
                     expand = c(0, 0), limits = 0:1)
```

If you subset the data, you'll see the study is sufficiently powered at $n = 93$.

```{r}
tibble(n = 6:100) %>% 
  mutate(se = sqrt(p * (1 - p) / n)) %>% 
  mutate(ll = p + -1.96 * se,
         ul = p + +1.96 * se) %>% 
  mutate(powered = ifelse(ll > .5, TRUE, FALSE)) %>% 
  filter(powered == TRUE)
```

Here's how this plays out if we use the different formula, $se = .5 / \sqrt{n}$.

```{r}
tibble(n = 6:100) %>% 
  mutate(se = .5 / sqrt(n)) %>% 
  mutate(ll = p + -1.96 * se,
         ul = p + +1.96 * se) %>% 
  mutate(powered = ifelse(ll > .5, TRUE, FALSE)) %>% 
  filter(powered == TRUE)
```

Thus, we would need $n > 96$ for adequate power. Yet,

> this is mistaken, however, because it confuses the assumption that $p = 0.6$ with the claim that $\hat p > 0.6$. In fact, if $p = 0.6$, then $\hat p$ depends on the sample, and it has an approximate normal distribution with mean $0.6$ and standard deviation $\sqrt{0.6 ∗ 0.4/n} = 0.49/ \sqrt n$ (p 294)

With that in mind, here's the top row of Figure 16.2.

```{r, fig.width = 8, fig.height = 3.5}
# left
p1 <-
  tibble(x = seq(from = .4, to = .8, length.out = 100)) %>% 
  mutate(d = dnorm(x, mean = .6, sd = 0.49 / sqrt(96))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_line() +
  scale_x_continuous(expression(hat(italic(p))), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = expression(distribution~of~hat(italic(p))~(based~on~italic(n)==96))) +
  theme(plot.subtitle = element_text(hjust = 0.5))

# right
set.seed(16)

p2 <-
  tibble(iter = 1:50) %>% 
  mutate(n = rbinom(n(), size = 96, p = .6)) %>% 
  mutate(p_hat = n / 96) %>% 
  mutate(se = sqrt(p_hat * (1 - p_hat) / 96)) %>% 
  mutate(ll = p_hat + -1.96 * se,
         ul = p_hat + +1.96 * se) %>% 
  mutate(powered = ifelse(ll > .5, TRUE, FALSE)) %>% 
  
  ggplot(aes(xmin = ll, xmax = ul, y = iter, group = iter)) + 
  geom_vline(xintercept = .5, linetype = 2, size = 1/4) +
  geom_linerange(aes(color = powered == TRUE),
                 size = 1/4) +
  scale_color_viridis_d(option = "A", end = .6, breaks = NULL) +
  scale_x_continuous(NULL, position = "top") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression("possible 95% intervals"~(based~on~italic(n)==96))) +
  theme(plot.subtitle = element_text(hjust = 0.5))

# combine
library(patchwork)
p1 + p2
```

> To determine the appropriate sample size, we must specify the desired *power*--that is, the probability that a $95 \%$ interval will be entirely above the comparison point of $0.5$. Under the assumption that $p = 0.6$, choosing $n = 96$ yields $50 \%$ power: there is a $50 \%$ chance that $\hat p$ will be more than $1.96$ standard deviations away from $0.5$, and thus a $50 \%$ chance that the $95 \%$ interval will be entirely greater than $0.5$. (p. 294, *emphasis* in the original)

If you look at the red and black interval lines in the right plot, above, you can get a sense of that $50 \%$ chance. Here's the sample behavior of $n = 10{,}000$ such intervals.

```{r}
set.seed(16)

tibble(iter = 1:1e4) %>% 
  mutate(n = rbinom(n(), size = 96, p = .6)) %>% 
  mutate(p_hat = n / 96) %>% 
  mutate(se = sqrt(p_hat * (1 - p_hat) / 96)) %>% 
  mutate(ll = p_hat + -1.96 * se,
         ul = p_hat + +1.96 * se) %>% 
  summarise(power = mean(ll > .5))
```

See? We're approaching the asymptotic behavior of $50 \%$ power.

> To find the value of n such that exactly 80% of the estimates will be at least 1.96 standard errors from 0.5, we need
>
> $$0.5 + 1.96 ∗ \text{s.e.} = 0.6 - 0.84 ∗ \text{s.e.}$$
>
> Some algebra then yields $(1.96 + 0.84) ∗ \text{s.e.}. = 0.1$. We can then substitute $\text{s.e.} = 0.5 / \sqrt n$ and solve for $n$, as we discuss next. (p. 294)

If you work through the equations, you might wonder where that $0.84 ∗ \text{s.e.}$ part came from. Well, consider what $-0.84$ is on the $z$ scale.

```{r}
pnorm(-0.84, mean = 0, sd = 1)
```

In standard normal distribution, $z = -0.84$ is the threshold dividing the lower $20 \%$ from the upper $80 \%$ of the distribution. Anyway, "in summary, *to have* $\textit{80%}$ *power, the true value of the parameter must be* $\textit{2.8}$ *standard errors away from the comparison point*" (p. 295, *emphasis* in the original). Here's where we get 2.8.

```{r}
1.96 + 0.84
```

Now in the text, we see: $n = (2.8 ∗ 0.49/0.1)^2 = 196$. This is incorrect and I suspect the authors meant $n = (2.8 ∗ 0.49/0.1)^2 = 188.2$ or $n = (2.8 ∗ 0.5/0.1)^2 = 196$. Look:

```{r}
(2.8 * 0.49 / 0.1)^2 
(2.8 * 0.5 / 0.1)^2 
```

Let's use our power workflow, from above, to compare the two sample sizes.

```{r}
# 189
sample_n <- 189

set.seed(16)

tibble(iter = 1:1e6) %>% 
  mutate(n = rbinom(n(), size = sample_n, p = .6)) %>% 
  mutate(p_hat = n / sample_n) %>% 
  mutate(se = sqrt(p_hat * (1 - p_hat) / sample_n)) %>% 
  mutate(ll = p_hat + -1.96 * se,
         ul = p_hat + +1.96 * se) %>% 
  summarise(power = mean(ll > .5))


# 196
sample_n <- 196

set.seed(16)

tibble(iter = 1:1e6) %>% 
  mutate(n = rbinom(n(), size = sample_n, p = .6)) %>% 
  mutate(p_hat = n / sample_n) %>% 
  mutate(se = sqrt(p_hat * (1 - p_hat) / sample_n)) %>% 
  mutate(ll = p_hat + -1.96 * se,
         ul = p_hat + +1.96 * se) %>% 
  summarise(power = mean(ll > .5))
```

It looks like $n = (2.8 ∗ 0.49/0.1)^2 = 188.2$ is the real deal. With that correction, here's the full reversion of Figure 16.2.

```{r, fig.width = 8, fig.height = 7}
# left
p3 <-
  tibble(x = seq(from = .4, to = .8, length.out = 100)) %>% 
  mutate(d = dnorm(x, mean = .6, sd = 0.49 / sqrt(189))) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_line() +
  scale_x_continuous(expression(hat(italic(p))), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  labs(subtitle = expression(distribution~of~hat(italic(p))~(based~on~italic(n)==189))) +
  theme(plot.subtitle = element_text(hjust = 0.5))

# right
set.seed(16)

p4 <-
  tibble(iter = 1:50) %>% 
  mutate(n = rbinom(n(), size = 189, p = .6)) %>% 
  mutate(p_hat = n / 189) %>% 
  mutate(se = sqrt(p_hat * (1 - p_hat) / 189)) %>% 
  mutate(ll = p_hat + -1.96 * se,
         ul = p_hat + +1.96 * se) %>% 
  mutate(powered = ifelse(ll > .5, TRUE, FALSE)) %>% 
  
  ggplot(aes(xmin = ll, xmax = ul, y = iter, group = iter)) + 
  geom_vline(xintercept = .5, linetype = 2, size = 1/4) +
  geom_linerange(aes(color = powered == TRUE),
                 size = 1/4) +
  scale_color_viridis_d(option = "A", end = .6, breaks = NULL) +
  scale_x_continuous(NULL, position = "top") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = expression("possible 95% intervals"~(based~on~italic(n)==189))) +
  theme(plot.subtitle = element_text(hjust = 0.5))

# combine
(p1 + p2) / (p3 + p4)
```

### 16.2.6 Estimates of hypothesized proportions.

When considering the standard error and it's size relative to $p$, we get the following relation:

```{r, fig.width = 4, fig.height = 2.75}
tibble(p = seq(from = .01, to = .99, by = .01)) %>% 
  mutate(numerator = sqrt(p * (1 - p))) %>% 
  
  ggplot(aes(x = p, y = numerator)) +
  geom_point() +
  labs(x = expression(italic(p)),
       y = expression(numerator/sqrt(italic(n))))
```

The exact value on the $y$-axis will depend on $n$. But you get a sense of the overall relation. Standard errors for proportions are largest when $p = .5$. Thus basing your standard errors on their values for when $p = .5$ will produce conservative estimates.

### 16.2.7 Simple comparisons of proportions: equal sample sizes.

> The standard error of a difference between two proportions is, by a simple probability calculation, $\sqrt{p_1 (1 - p_1) / n_1 + p_2 (1 - p_2) / n_2}$, which has an upper bound of $0.5 \sqrt{1 / n_1 + 1 / n_2}$. If we assume $n_1 = n_2 = n / 2$ (equal sample sizes in the two groups), the upper bound on the standard error becomes simply $1/ \sqrt n$. A specified standard error can then be attained with a sample size of $n = 1 / (\text{s.e.})^2$. If the goal is $80 \%$ power to distinguish between hypothesized proportions $p_1$ and $p_2$ with a study of size $n$, equally divided between the two groups, a conservative sample size is $n = ((2.8 / (p_1 - p_2))^2$ or, more precisely, $n = 2 (p_1 (1 - p_1) + p_2 (1 - p_2))(2.8 / (p_1 - p_2))^2$. (p. 296)

Here's how to use the conservative equation to compute the necessary total sample size $N$ for an expected difference of $10 \%$ between two equal-sized groups. 

```{r}
# total N
(2.8 / .1)^2

# per group
(2.8 / .1)^2 / 2
```

### 16.2.8 Simple comparisons of proportions: unequal sample sizes.

Using the equation, $N = (1.25 / \text{s.e.})^2$, we can plot the relation for the total sample size $N$ for a given standard error like this.

```{r, fig.width = 4, fig.height = 2.75}
tibble(se = seq(from = .1, to = .9, by = .01)) %>% 
  mutate(n = (1.25 / se)^2) %>% 
  
  ggplot(aes(x = se, y = n)) +
  geom_point() +
  scale_x_continuous(expand = c(0, 0), limits = 0:1, breaks = 0:5 / 5) +
  scale_y_continuous(expression(italic(N)), limits = c(0, NA),
                     expand = expansion(mult = c(0, 0.05)))
```

If you have an expected difference in proportion of $10 \%$ for groups you anticipate will have an $n_1 / n_2 = 1/5$ split, here's how to compute the total and group-specific sample sizes.

```{r}
# total N
(2.8 * 1.25 / 0.1)^2

# 20/80% split
(2.8 * 1.25 / 0.1)^2 * c(0.2, 0.8)
```

## 16.3 Sample size and design calculations for continuous outcomes














## Session info {-}

```{r}
sessionInfo()
```

```{r, warning = F, echo = F, eval = F}
rm(list = ls())
```

```{r, echo = F, message = F, warning = F, results = "hide", eval = F}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

