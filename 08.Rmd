---
title: "Chapter 8: Fitting regression models"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

```{r, eval = F, echo = F}
https://github.com/avehtari/ROS-Examples/
```

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
# options(width = 100)
```

# Fitting regression models

> In this chapter we lay out some of the mathematical structure of inference for regression models and some algebra to help understand estimation for linear regression. We also explain the rationale for the use of the Bayesian fitting routine [`brm()`] and its connection to classical linear regression. This chapter thus provides background and motivation for the mathematical and computational tools used in the rest of the book. (p. 103)

## 8.1 Least squares, maximum likelihood, and Bayesian inference

> We now step back and consider *inference*: the steps of estimating the regression model and assessing uncertainty in the fit. We start with *least squares*, which is the most direct approach to estimation, based on finding the values of the coefficients $a$ and $b$ that best fit the data. We then discuss *maximum likelihood*, a more general framework that includes least squares as a special case and to which we return in later chapters when we get to logistic regression and generalized linear models. Then we proceed to *Bayesian inference*, an even more general approach that allows the probabilistic expression of prior information and posterior uncertainty. (p. 103, *emphasis* in the original)

### 8.1.1 Least squares.

> In the classical linear regression model, $y_i = a + b x_i + \epsilon_i$, the coefficients $a$ and $b$ are estimated so as to minimize the errors $\epsilon_i$. If the number of data points $n$ is greater than 2, it is not generally possible to find a line that gives a perfect fit (that would be $y_i = a + b x_i$ , with no error, for all data points $i = 1, \dots , n$), and the usual estimation goal is to choose the estimate ($\hat a$, $\hat b$) that minimizes the sum of the squares of the residuals,
> 
> $$r_i = y_i − (\hat a + b x_i) .$$
> 
> We distinguish between the residuals $r_i = y_i - (\hat a + \hat b x_i)$ and the *errors* $\epsilon_i = y_i − (a + b x_i)$.
> 
> The model is written in terms of the errors, but it is the residuals that we can work with: we cannot calculate the errors as to do so would require knowing $a$ and $b$.
> 
> The residual sum of squares is
> 
> $$\text{RSS} = \sum_{i=1}^n (y_i = (\hat a + \hat b x_i))^2.$$
> 
> The ($\hat a$, $\hat b$) that minimizes RSS is called the least squares or ordinary least squares or OLS estimate. (p. 103, *emphasis* in the original)

### 8.1.2 Estimation of residual standard deviation $\sigma$.

> In the regression model, the errors $\epsilon_i$ come from a distribution with mean 0 and standard deviation $\sigma$: the mean is zero by definition (any nonzero mean is absorbed into the intercept, $a$), and the standard deviation of the errors can be estimated the from data. A natural way to estimate $\sigma$ would be to simply take the standard deviation of the residuals, $\sqrt{\frac{1}{n} \sum_{i=1}^n r_i^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n y_i - (\hat a + \hat b x_i))^2}$, but this would slightly underestimate $\sigma$ because of *overfitting*, as the coefficients $\hat a$ and $\hat b$ have been set based on the data to minimize the sum of squared residuals. The standard correction for this overfitting is to replace $n$ by $n - 2$ in the denominator (with the subtraction of 2 coming from the estimation of two coefficients in the model, the intercept and the slope); thus,
> 
> $$\hat \sigma = \sqrt{\frac{1}{n-2} \sum_{i=1}^n (y_i - (\hat a + \hat b x_i))^2}.$$
> 
> When $n = 1$ or $2$ this expression is meaningless, which makes sense: with only two data points you can fit a line exactly and so there is no way of estimating error from the data alone. (p. 104, *emphasis* in the original)

### 8.1.3 Computing the sum of squares directly.

Here we make a custom function to compute the sum of squares for different values of $a$ and $b$.

```{r}
rss <- function(x, y, a, b) {  
  
  # x and y are vectors, 
  # a and b are scalars 
  
  resid <- y - (a + b * x)
  
  return(sum(resid^2))

  }
```

We can use this with `hibbs` data from Chapter 7.

```{r, warning = F, message = F}
library(tidyverse)

hibbs <- 
  read_table2("ROS-Examples-master/ElectionsEconomy/data/hibbs.dat") %>%
  mutate(inc_party_candidate = str_remove_all(inc_party_candidate, '[\"]'),
         other_candidate     = str_remove_all(other_candidate, '[\"]'))

hibbs
```

As in model `m7.1` from last chapter, we'll designate `vote` as the $y$ variable and $x$ as the predictor. We'll first compute the residual sum of squares based on the formula $y_i = 46.3 + 3.0 x_i + \epsilon_i$.

```{r}
rss(hibbs$growth, hibbs$vote, 46.3, 3.0)
```

We might explore with different values of `a` and `b`. To start out, we'll try a vector of `a` values, while retaining `b = 3.0`. We'll then plot the results.

```{r, warning = F, fig.width = 3.75, fig.height = 3.25}
# set the global plotting theme
theme_set(theme_linedraw() +
            theme(panel.grid = element_blank()))

# simulate
tibble(a = 30:60) %>% 
  mutate(rss = map_dbl(a, rss, x = hibbs$growth, y = hibbs$vote,  b = 3.0)) %>% 
  
  # plot
  ggplot(aes(x = a, y = rss)) +
  geom_point() +
  labs(subtitle = "b is held constant at 3.0")
```

Now we'll vary both `a` and `b`, each across a continuum of parameters and plot the results in a tile plot where the fill of each tile is the `rss` value.

```{r, warning = F, fig.width = 4.75, fig.height = 3.25}
# simulate
d <-
  crossing(a = seq(from = 30, to = 60, by = 0.1),
           b = seq(from = 0, to = 10, by = 0.05)) %>% 
  mutate(rss = map2_dbl(a, b, rss, x = hibbs$growth, y = hibbs$vote))

d %>%
  # plot
  ggplot(aes(x = a, y = b, fill = rss)) +
  geom_tile() +
  scale_fill_viridis_c("RSS", option = "A") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
```

Since ordinary least squares estimation emphasizes the smallest RSS value, we like the combinations of `a` and `b` in the darker range. Among the values we entertained, here is the combination of `a` and `b` with the lowest `rss`.

```{r}
d %>% 
  arrange(rss) %>% 
  slice(1)
```

### 8.1.4 Maximum likelihood.

> If the errors from the linear model are independent and normally distributed, so that $y_i \sim \operatorname N (a + b x_i, \sigma^2)$ for each $i$, then the least squares estimate of ($a$, $b$) is also the maximum likelihood estimate. The *likelihood function* in a regression model is defined as the probability density of the data given the parameters and predictors; thus, in this example,
> 
> $$p(y | a, b, \sigma, X) = \prod_{i=1}^n \operatorname N(y_i | a + b x_i, \sigma^2),$$
> 
> where $\operatorname N(\cdot | \cdot, \cdot)$ is the normal probability density function,
> 
> $$\operatorname N(y | m, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma}} \exp \left(-\frac{1}{2} \left( \frac{y - m}{\sigma} \right)^2 \right).$$
> 
> A careful study of (8.6) reveals that maximizing the likelihood requires minimizing the sum of squared residuals; hence the least squares estimate $\hat \beta = (\hat a, \hat b)$ can be viewed as a maximum likelihood estimate under the normal model. (p. 105, *emphasis* in the original)

### 8.1.5 Where do the standard errors come from? Using the likelihood surface to assess uncertainty in the parameter estimates.

For the sake of practice, here we'll use the equations from the last section to make a custom function to compute the log-likelihood estimates for various combinations of `a` and `b`, given two data columns, `x` and `y`. It'll return the $\hat \sigma$ value, too.

```{r}
ll <- function(x, y, a, b) {
  
  # compute sigma
  resid <- y - (a + b * x)
  sigma <- sqrt(sum(resid^2) / length(x))
  
  d <- dnorm(y, mean = a + b * x, sd = sigma, log = T)
  
  tibble(sigma = sigma,
         ll = sum(d))
  
}
```

Now we'll iterate this over many values of `a` and `b`, save the results, and make a plot.

```{r, warning = F, fig.width = 3.75, fig.height = 3.25}
# simulate
d <-
  crossing(a = seq(from = 39, to = 53, length.out = 200),
           b = seq(from = 0, to = 6, length.out = 200)) %>% 
  mutate(ll = map2(a, b, ll, x = hibbs$growth, y = hibbs$vote)) %>% 
  unnest(ll)

# plot
p1 <-
  d %>%
  ggplot(aes(x = a, y = b, fill = ll)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A", breaks = NULL) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(subtitle = "likelihood, p(a, b |y)")

p1
```

This will also serve as our Figure 8.1a. Based on the values we considered, here's the maximum likelihood.

```{r}
d %>% 
  arrange(desc(ll)) %>% 
  slice(1)
```

The authors have not yet covered maximum likelihood standard errors, so we aren't in a position to make versions of Figure 8.1b or 6.1c. But we can make Figure 8.2. First we'll need to bring back model `m7.1` from Section [7.1.1][Fitting a linear model to data.].

```{r m7.1_08, warning = F, message = F}
library(brms)

m7.1 <-
  brm(data = hibbs,
      vote ~ growth,
      seed = 7,
      file = "fits/m07.01")
```

Now we make Figure 8.2a.

```{r, warning = F, fig.width = 3.75, fig.height = 3.25}
hibbs %>% 
  ggplot(aes(x = growth, y = vote, label = year)) +
  geom_point() +
  geom_abline(intercept = fixef(m7.1, robust = T)[1, 1], 
              slope = fixef(m7.1, robust = T)[2, 1],
              size = 1/3) +
  annotate(geom = "text",
           x = 3.5, y = 53.5,
           label = expression(y==46.2+3.1*x)) +
  labs(subtitle = "Data and linear fit",
       x = "x",
       y = "y")
```

Here's Figure 8.2b.

```{r, warning = F, fig.width = 3.75, fig.height = 3.25}
set.seed(8)

posterior_samples(m7.1) %>% 
  slice_sample(n = 50) %>% 
  
  ggplot() +
  geom_abline(aes(intercept = b_Intercept, slope = b_growth),
              size = 1/4, alpha = 1/2, color = "grey25") +
  geom_point(data = hibbs,
             aes(x = growth, y = vote)) +
  scale_x_continuous(breaks = 0:4, limits = c(-1, 5), expand = c(0, 0)) +
  coord_cartesian(ylim = c(43, 62)) +
  labs(subtitle = "Data and range of possible linear fits",
       x = "x",
       y = "y")
```

What was this magic. We used the `brms::posterior_samples()` function to take many draws from the posterior distribution of model `m7.1`. That left us with a column of each of the three parameters, which in this output are called `b_Intercept`, `b_growth`, and `sigma`. We then used `slice_sample()` to take a random subset of 50 of the rows. To keep things simple, here we'll take only 5.

```{r}
set.seed(8)

posterior_samples(m7.1) %>% 
  slice_sample(n = 5)
```

We also got a column called the `lp__`, which is currently outside of our scope. But anyway, we can look at the `b_Intercept` as a collection of credible `a` values and the `b_growth` column as a collection of credible `b` values.

### 8.1.6 Bayesian inference.

> Least squares or maximum likelihood finds the parameters that best fit the data (according to some pre-specified criterion), but without otherwise constraining or guiding the fit. But, as discussed in Section 9.3 and elsewhere, we typically have prior information about the parameters of the model. Bayesian inference produces a compromise between prior information and data, doing this by multiplying the likelihood with a *prior distribution* that probabilistically encodes external information about the parameters. The product of the likelihood (in the above example, it is $p(y | a, b, \sigma)$ in (8.6), considered as a function of $a$, $b$, and $\sigma$) and the prior distribution is called the *posterior distribution* and it summarizes our knowledge of the parameter, after seeing the data. ("Posterior" is Latin for "later.") (p. 106, *emphasis* in the original)

### 8.1.7 Point estimate, mode-based approximation, and posterior simulations.

> The least squares solution is a *point estimate* that represents the vector of coefficients that provides the best overall fit to data. For a Bayesian model, the corresponding point estimate is the *posterior mode*, which provides the best overall fit to data and prior distribution. The least squares or maximum likelihood estimate is the posterior mode corresponding to the model with a uniform or flat prior distribution.
> 
> But we do not just want an estimate; we also want uncertainty. (p. 107, *emphasis* in the original)

## 8.2 Influence of individual points in a fitted regression

It's not clear where the data came from to make Figure 8.3. We'll have to use the skills from previous figures to improvise. To start, we define the population parameters for `a`, `b`, and `sigma`. Then we simulate values of `x` ranging from 1 t0 13 and the corresponding values for `y`, `y_hat`, and even `r_hat`. Then we plot.

```{r, warning = F, fig.width = 3.75, fig.height = 3.25}
a <- 3
b <- 2
sigma <- 7

set.seed(8)

tibble(x = 1:13) %>% 
  mutate(y_hat = a + b * x,
         r_hat = rnorm(n(), mean = 0, sd = sigma)) %>% 
  mutate(y = y_hat + r_hat) %>% 
  
  ggplot(aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = y_hat),
            size = 1/2) +
  geom_linerange(aes(ymin = y, ymax = y_hat),
                 size = 1/4) +
  scale_x_continuous(breaks = 1:6 * 2) +
  coord_cartesian(xlim = c(1.9, 12.1),
                  ylim = c(-3, 33))
```

> An increase of 1 in $y_i$ corresponds to a change in $\hat b$ that is proportional to ($x_i - \bar x$):
> 
> * If $x_i = \bar x$, the influence of point $i$ on the regression slope is 0. This makes sense: taking a point in the center and moving it up or down will affect the height of the fitted line but not its slope.
> * If $x_i > \bar x$, the influence of point $i$ is positive, with greater influence the further $x_i$ is from the mean.
> * If $x_i < \bar x$, the influence of point $i$ is negative, with greater absolute influence the further $x_i$ is from the mean. (p. 107)

## 8.3 Least squares slope as a weighted average of slopes of pairs

"We can interpret the estimated coefficient $\hat b$ as the weighted average slope in the data, and we can interpret the underlying parameter $b$ as the weighted average slope in the population" (p. 109)

## 8.4 Comparing two fitting functions: `lm` and ~~`stan_glm`~~ `brm`

The base **R** `lm()` function fits models using the OLS criterion. Throughout the text, Gelman et al highlighted the `stan_glm()` function, which is part of the [**rstanarm** package](https://CRAN.R-project.org/package=rstanarm). From the [https://mc-stan.org/rstanarm/](https://mc-stan.org/rstanarm/) website, the developers describe **rstanarm** as:

> an R package that emulates other R model-fitting functions but uses Stan (via the [rstan](https://mc-stan.org/rstan/) package) for the back-end estimation. The primary target audience is people who would be open to Bayesian inference if using Bayesian software were easier but would use frequentist software otherwise.
>
> Fitting models with **rstanarm** is also useful for experienced Bayesian software users who want to take advantage of the pre-compiled Stan programs that are written by Stan developers and carefully implemented to prioritize numerical stability and the avoidance of sampling problems.

In this project, we will be use the **brm** package, instead. From the [https://github.com/paul-buerkner/brms](https://github.com/paul-buerkner/brms) page, we see **brms** described as:

> The **brms** package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see [https://mc-stan.org/](https://mc-stan.org/)). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors.

Though **brms** and **rstanarm** are very similar, **brms** offers a more flexible model-fitting framework. As we have already seen, the primary model-fitting function with **brms** is `brm()`, which might be thought of as an acronym for *Bayesian regression model*. Similar to `rstanarm::stan_glm()`, the `brms::brm()` uses fairly weak priors, by default. However, the two do not necessarily use the same default priors, so some differences may arise when relying on those defaults.

### 8.4.1 Reproducing maximum likelihood using ~~`stan_glm`~~ `brm` with flat priors and optimization.

Similar to with `stan_glm()`, the default use of `brms::brm()` is

```{r, eval = F}
brm(data = mydata,
    y ~ x)
```

It doesn't matter what order you put the arguments in. But you'll note I prefer to define my data before defining my model. Even when you don't set them yourself, `brm()` assigns priors to all model parameters, by default. If you are ever curious about what those priors are, use the `get_prior()` function.

**brms** categorizes priors into various classes. Priors for regression slopes are often of `class = b`. By default, parameters of `class = b` have a flat prior across the real number line. The priors for model intercepts, however, are typically of `class = Intercept`. These default to the normal distribution wherein the $\mu$ and $\sigma$ parameters are automatically set to be wide and minimally informative. For more details, see the `set_prior` section of the [**brms** reference manual](https://CRAN.R-project.org/package=brms/brms.pdf).

Unlike with `stan_glm()`, the `brm()` function does not have an `algorithm = "optimizing"` that will allow for maximum likelihood estimation. If you want frequentist estimation, **brms** will be of little help, for you.

### 8.4.2 Running `lm`.

If you would like to fit a maximum likelihood model, the `lm()` function will often get you an equivalent model using OLS.

### 8.4.3 Confidence intervals, uncertainty intervals, compatibility intervals.

Simulate some data.

```{r}
fake <- tibble(x = 1:10, 
               y = c(1, 1, 2, 3, 5, 8, 13, 21, 34, 55))

glimpse(fake)
```

Fit the model with `brms::brm()`.

```{r m8.1}
m8.1 <-
  brm(data = fake,
      y ~ x,
      seed = 8,
      file = "fits/m08.01")
```

Check the model summary.

```{r}
print(m8.1, robust = T)
```

By default, the summary includes percentile-based 95% intervals for all parameters in the model. Those are listed in the columns called 'l-95% CI' and 'u-95% CI'. These are the same as if you were to use the base **R** `quantlie()` function, which we'll do by our selves in a bit. If you'd like to extract the simulation draws from a **brms** model, the go-to function is `posterior_samples()`. Here we'll save those draws as a data frame called `post`.

```{r}
post <- posterior_samples(m8.1)

head(post)
```

We have three columns, named after the `a`, `b`, and `sigma` parameters, respectively. As we briefly covered in [Section 8.1.5][Where do the standard errors come from? Using the likelihood surface to assess uncertainty in the parameter estimates.], the final `lp__` column will be outside of the scope of this project. Just know that it'll always be there, looming in the shadows. Here's how we might hand-compute the percentile-based 95% intervals using `quantile()`.

```{r, message = F}
post %>% 
  pivot_longer(b_Intercept:sigma) %>% 
  group_by(name) %>% 
  summarise(`2.5%`  = quantile(value, prob = .025),
            `97.5%` = quantile(value, prob = .975))
```

Another handy way to get them is with the `posterior_summary()` function.

```{r}
posterior_summary(m8.1)
```

The intervals are listed in the `Q2.5` and `Q97.5` columns. The `posterior_interval()` function returns even more focused output.

```{r}
posterior_interval(m8.1)
```

## Session info {-}

```{r}
sessionInfo()
```

```{r, warning = F, echo = F, eval = F}
rm(list = ls())
```

```{r, echo = F, message = F, warning = F, results = "hide", eval = F}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

