---
title: "Chapter 19: Causal inference using regression on the treatment variable"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

```{r, eval = F, echo = F}
https://github.com/avehtari/ROS-Examples/
```

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
# options(width = 100)
```

# Causal inference using regression on the treatment variable

> This chapter illustrates the use of regression in the setting of controlled experiments, going through issues of adjustment for pre-treatment predictors, interactions, and pitfalls that can arise when building a regression using experimental data and interpreting coefficients causally. (p. 363)

## 19.1 Pre-treatment covariates, treatments, and potential outcomes

Three kinds of data are

* pre-treatment measurements (*covariates*),
* treatment status ($z_i$), and
* potential outcomes ($y_i$).

## 19.2 Example: the effect of showing children an educational television show

Load the `electric.csv` data.

```{r, warning = F, message = F}
library(tidyverse)
library(tidybayes)

electric <- read_csv("ROS-Examples-master/ElectricCompany/data/electric.csv")

head(electric)
```

### 19.2.1 Displaying the data two different ways.

To help get a sense of the data, here are the `X1`-level trajectories for the `pre_test` and `post_test` scores, faceted by `grade` and `treatment`.

```{r, fig.width = 4, fig.height = 5}
# set the global plotting theme
theme_set(theme_linedraw() +
            theme(panel.grid = element_blank()))

# wrangle
electric %>% 
  pivot_longer(ends_with("test"), values_to = "score") %>% 
  mutate(grade     = str_c("grade ", grade),
         time      = ifelse(name == "pre_test", 0, 1),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("control", "treatment"))) %>% 
  
  # plot
  ggplot(aes(x = time, y = score, group = X1, color = treatment)) +
  geom_line(size = 1/6) +
  scale_color_viridis_d(option = "A", end = .67, breaks = NULL) +
  scale_x_continuous(NULL, limits = c(-0.2, 1.2), 
                     breaks = 0:1, labels = c("pre_test", "post_test")) +
  facet_grid(grade~treatment)
```

Before we make Figure 19.2, which focuses on the `post_test` distributions, we'll first want to make a data set summarizing the mean and sd values.

```{r, message = F}
text <-
  electric %>% 
  mutate(grade     = str_c("Grade ", grade),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("Test scores in control classes", "Test scores in treated classes"))) %>% 
  group_by(grade, treatment) %>% 
  summarise(mean = mean(post_test),
            sd   = sd(post_test)) %>% 
  mutate_if(is.double, round, digits = 0) %>% 
  mutate(mean = str_c("mean = ", mean),
         sd   = str_c("sd = ", sd)) %>% 
  pivot_longer(mean:sd, values_to = "label") %>% 
  mutate(post_test = if_else(name == "mean", 5, 10),
         count     = if_else(name == "mean", 8, 6))

text
```

Now make Figure 19.2 with help from `facet_grid()`, which will divide the plot into a $4 \times 2$ grid.

```{r, fig.width = 5, fig.height = 4.5}
# wrangle
electric %>% 
  mutate(grade     = str_c("Grade ", grade),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("Test scores in control classes", "Test scores in treated classes"))) %>% 
  
  # plot
  ggplot(aes(x = post_test)) +
  geom_histogram(boundary = 0, binwidth = 5,
                 fill = "grey75", color = "black", size = 1/4) +
  geom_text(data = text,
            aes(y = count, label = label),
            hjust = 0) +
  scale_x_continuous(NULL, breaks = 0:2 * 50, 
                     limits = c(0, 125), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  theme(axis.ticks.x = element_blank()) +
  facet_grid(grade~treatment)
```

To visualize the data as in Figure 19.3, we'll want to first make a `vline` tibble that has the mean values for each of the `grade` by `treatment` groupings.

```{r, message = F}
vline <-
  electric %>% 
  mutate(grade     = str_c("Grade ", grade),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("Control classes", "Treated classes"))) %>% 
  group_by(grade, treatment) %>% 
  summarise(mean = mean(post_test))

vline
```

Now make Figure 19.3.

```{r, fig.width = 7, fig.height = 2.5}
# wrangle
electric %>% 
  mutate(grade     = str_c("Grade ", grade),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("Control classes", "Treated classes"))) %>% 
  
  # plot
  ggplot(aes(x = post_test)) +
  geom_histogram(boundary = 0, binwidth = 5,
                 fill = "grey75", color = "black", size = 1/4) +
  geom_vline(data = vline,
             aes(xintercept = mean),
             size = 1, color = "royalblue") +
  scale_x_continuous(NULL, breaks = 2:4 * 25, 
                     limits = c(40, 125), expand = c(0, 0)) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  theme(axis.ticks.x = element_blank()) +
  facet_grid(treatment~grade)
```

### 19.2.2 Paired comparisons design.

The study used a a matched pairs design, which is depicted in the `pair_id` column.

```{r}
electric %>% 
  count(pair_id)
```

However, Gelman et al adopted to keep things simple and analyze these data without accounting for this pairing.

### 19.2.3 Simple difference estimate (equivalently, regression on an indicator for treatment), appropriate for a completely randomized experiment with no pre-treatment variables.

We start by fitting the simple model

$$
\begin{align*}
\text{post_test}_i & \sim \operatorname N(\mu_i, \sigma) \\
\mu_i & = a + b\ \text{treatment}_i.
\end{align*}
$$

If we use more generic notation where the post-intervention criterion is $y_i$, the treatment is denoted $z_i$, and the coefficient for the treatment effect is $\theta$, we might re-write the model as

$$
\begin{align*}
y_i & \sim \operatorname N(\mu_i, \sigma) \\
\mu_i & = a + \theta z_i.
\end{align*}
$$

Fit the model with **brms**.

```{r m19.1, warning = F, message = F}
library(brms)

m19.1 <-
  brm(data = electric,
      post_test ~ treatment,
      seed = 19,
      file = "fits/m19.01")
```

Check the model summary.

```{r}
print(m19.1, robust = T)
```

### 19.2.4 Separate analysis within each grade.

To serially fit the model individually by `grade`, we'll use the `purrr::map()` approach to iteration.

```{r m19.1a, results = "hide", message = F}
fits1 <-
  electric %>% 
  nest(data = c(X1, post_test, pre_test, treatment, supp, pair_id)) %>% 
  mutate(fit = map(data, ~update(m19.1,
                                 newdata = .,
                                 seed = 19)))
```

Here's how we might work with output to make the left panel of Figure 19.4.

```{r, fig.width = 4, fig.height = 1.75}
p1 <-
  fits1 %>% 
  mutate(post = map(fit, posterior_samples)) %>% 
  select(-data, -fit) %>% 
  unnest(post) %>% 
  mutate(grade = factor(str_c("Grade ", grade),
                        levels = str_c("Grade ", 4:1))) %>% 
  
  ggplot(aes(x = b_treatment, y = grade)) +
  geom_vline(xintercept = 0, linetype = 2, size = 1/4) +
  stat_pointinterval(.width = c(.5, .95)) +
  scale_x_continuous("Regression on treatment indicator", position = "top") +
  ylab("Subpopulation") +
  theme(axis.ticks.y = element_blank())

p1
```

## 19.3 Including pre-treatment predictors

### 19.3.1 Adjusting for pre-test to get more precise estimates.

Now we'll fit the following model

$$
\begin{align*}
\text{post_test}_i & \sim \operatorname N(\mu_i, \sigma) \\
\mu_i & = a + b\ \text{treatment}_i + \text{pre_test}_i,
\end{align*}
$$

separately by `grade`, where the treatment effect is conditioned on `pre_test` scores.

```{r m19.1b, results = "hide", message = F}
fits2 <-
  electric %>% 
  nest(data = c(X1, post_test, pre_test, treatment, supp, pair_id)) %>% 
  mutate(fit = map(data, ~update(m19.1,
                                 newdata = .,
                                 formula = post_test ~ treatment + pre_test,
                                 seed = 19)))
```

Now make the complete version of Figure 19.4.

```{r, fig.width = 8, fig.height = 2}
p2 <-
  fits2 %>% 
  mutate(post = map(fit, posterior_samples)) %>% 
  select(-data, -fit) %>% 
  unnest(post) %>% 
  mutate(grade = factor(str_c("Grade ", grade),
                        levels = str_c("Grade ", 4:1))) %>% 
  
  ggplot(aes(x = b_treatment, y = grade)) +
  geom_vline(xintercept = 0, linetype = 2, size = 1/4) +
  stat_pointinterval(.width = c(.5, .95)) +
  scale_x_continuous("Regression on treatment indicator,\ncontrolling for pre", position = "top") +
  scale_y_discrete(NULL, breaks = NULL) +
  theme(axis.ticks.y = element_blank())

# combine
library(patchwork)
(p1 + p2) &
  coord_cartesian(xlim = c(-3.8, 17))
```

Now the treatment effects, $\theta$, are more orderly and precise. Before we make our version of Figure 19.5, we'll want to summarize the posteriors by the intercept and slopes, with respect to `pre_test`.

```{r}
abline <-
  fits2 %>% 
  mutate(fixef = map(fit, ~fixef(.) %>% 
                       data.frame() %>% 
                       rownames_to_column("parameter"))) %>% 
  unnest(fixef) %>% 
  select(grade, parameter, Estimate) %>% 
  pivot_wider(names_from = parameter, values_from = Estimate) %>% 
  set_names("grade", "a", "theta", "b") %>% 
  expand(nesting(grade, a, theta, b),
         treatment = 0:1) %>% 
  mutate(intercept = a + theta * treatment,
         slope = b) %>% 
  mutate(treatment = factor(treatment),
         grade     = str_c("grade ", grade))

abline
```

Now make Figure 19.5.

```{r, fig.width = 8, fig.height = 2.5}
electric %>% 
  mutate(treatment = factor(treatment),
         grade     = str_c("grade ", grade)) %>% 
  
  ggplot(aes(x = pre_test, y = post_test)) +
  geom_abline(data = abline,
              aes(intercept = intercept, slope = slope, linetype = treatment),
              size = 1/4) +
  geom_point(aes(fill = treatment),
             shape = 21, stroke = 1/4) +
  scale_linetype_manual(values = 2:1, breaks = NULL) +
  scale_fill_manual(values = c("black", "transparent"), breaks = NULL) +
  scale_x_continuous(expression("pre-test, "*italic(x[i])), 
                                breaks = 0:3 * 40, limits = c(0, 125)) +
  scale_y_continuous(expression("post-test, "*italic(y[i])),
                     breaks = 0:3 * 40, limits = c(0, 125)) +
  facet_wrap(~grade, nrow = 1)
```

The big difference in slopes for first grade versus all other grades is the pre-test measure was based on a subset of the post-test questions.

### 19.3.2 Benefits of adjusting for pre-treatment score.

> To get a sense of what we get by adjusting for a pre-treatment predictor, suppose that in a particular grade the average pre-test score is $\Delta_x$ points higher for the treatment than the control group. Such a difference would not necessarily represent a failure of assumptions; it could just be chance variation that happened to occur in this particular randomization. In any case, *not* adjusting for this pre-treatment imbalance would be a mistake: scores on pre-test and post-test are positively correlated, and so the unadjusted comparison would tend to overestimate the treatment effect by an amount $b \Delta_x$ , in this case. Performing the regression automatically performs this adjustment on the estimate of $\theta$. (p. 368, *emphasis* in the original)

### 19.3.3 Problems with simple before-after comparisons.

> Given that we have pre-test and post-test measurements, why not simply summarize the treatment effect by their difference? Why bother with a controlled experiment at all? The problem with the simple before-after estimate is that, when estimating causal effects we are interested in the difference between treatment and control conditions, not in the simple improvement from pre-test to post-test. The improvement is not a causal effect (except under the assumption, unreasonable in this case, that under the control there would be no change in reading ability during the school year). (p 369)

### 19.3.4 Gain scores: a special case of regression in which the coefficient for pre-test is fixed at 1.

We can compute a gain score ($g_i$) by subtracting the pre-intervention score (x_i) from the post-intervention score ($y_i$), 

$$g_i = y_i - x_i.$$

With the `electric` data, this would be

$$\text{gain}_i = \text{post_test}_i - \text{pre_test}_i.$$

Why not make a `gain` column?

```{r}
electric <-
  electric %>% 
  mutate(gain = post_test - pre_test)
```

To get a sense of `gain`, make a $2 \times 3$ grid of histograms.

```{r, fig.width = 5, fig.height = 2.75}
# wrangle
electric %>% 
  filter(grade > 1) %>% 
  mutate(grade     = str_c("Grade ", grade),
         treatment = factor(treatment, 
                            levels = 0:1,
                            labels = c("Control classes", "Treated classes"))) %>% 
  
  # plot
  ggplot(aes(x = gain)) +
  geom_histogram(boundary = 0, binwidth = 2.5,
                 fill = "grey75", color = "black", size = 1/4) +
  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +
  xlab("gain score") +
  theme(axis.ticks.x = element_blank()) +
  facet_grid(treatment~grade)
```

To practice the gain score approach, here we'll fit the model

$$
\begin{align*}
g_i & \sim \operatorname N(\mu_i, \sigma) \\
\mu_i & = \alpha + \tau z_i,
\end{align*}
$$

where $z_i$ is the treatment assignment and $\tau$ is the causal effect. To keep things simple, we'll only fit the model for grade 2.

```{r m19.2, warning = F, message = F}
m19.2 <-
  brm(data = electric %>% filter(grade == 2),
      gain ~ treatment,
      seed = 19,
      file = "fits/m19.02")
```

Check the model summary.

```{r}
print(m19.2, robust = T)
```

If the estimate for the treatment effect for the model we just fit, 

$$g_i = \alpha + \tau z_i + \text{error}_i,$$

can be expressed as 

$$\hat \tau = \bar g^T - \bar g^C,$$

then we might compare our $\hat \tau$ posterior median, above, with the sample statistics.

```{r, message = F}
electric %>% 
  filter(grade == 2) %>% 
  group_by(treatment) %>% 
  summarise(g_bar = mean(gain)) %>% 
  pivot_wider(names_from = treatment, values_from = g_bar) %>% 
  summarise(tau_hat = `1` - `0`)
```

However, 

> one perspective on the analysis of gain scores is that it implicitly makes an unnecessary assumption, namely, that $\beta = 1$ in model (19.1). To see this, note the algebraic equivalence between $y_i = \alpha + \tau z_i + x_i + \text{error}_i$ and $y_i - x_i= \alpha + \tau z_i + \text{error}_i$. On the other hand, if this assumption is close to being true, then $\tau$ may be estimated more precisely. (p. 369)

For comparison, here are the $\beta$ parameters from the `fits2` models from above.

```{r}
fits2 %>% 
  mutate(fixef = map(fit, ~fixef(., robust = T) %>% 
                       data.frame() %>% 
                       rownames_to_column("parameter"))) %>% 
  unnest(fixef) %>% 
  select(-data, -fit) %>% 
  filter(grade > 1 & parameter == "pre_test") %>% 
  mutate_if(is.double, round, digits = 2)
```

The posteriors were somewhat close to 1.

> One way to resolve this concern about misspecification would simply be to include the pre-test score as a predictor as well, $g_i = \alpha + \tau z_i + \gamma x_i + \text{error}_i$. However, in this case, $\hat \tau$, the estimate of the coefficient for $z$, is equivalent to the estimated coefficient from the original model, $y_i = \alpha + \tau z_i + \beta x_i + \text{error}_i$. (p. 369)

Let's see.

```{r m19.3, warning = F, message = F}
m19.3 <-
  brm(data = electric %>% filter(grade == 2),
      gain ~ treatment + pre_test,
      seed = 19,
      file = "fits/m19.03")
```

Check the model summary.

```{r}
print(m19.3, robust = T)
```

Now compare the `treatment` posterior with the posterior summary from the model from earlier.

```{r}
fits2 %>% 
  mutate(fixef = map(fit, ~fixef(., robust = T) %>% 
                       data.frame() %>% 
                       rownames_to_column("parameter"))) %>% 
  unnest(fixef) %>% 
  select(-data, -fit) %>% 
  filter(grade == 2 & parameter == "treatment") %>% 
  mutate_if(is.double, round, digits = 2)
```

Yep, they're the same within simulation variance.

> Another motivation for use of gain scores is the desire to interpret effects on changes in the outcome rather than the effect on the outcome on its own. Compare this interpretation to the interpretation of a treatment effect estimate from a model that adjusts for the pre-test; in this case we could interpret an effect on the outcome for those with the same value of the pre-test. The difference between these interpretations is subtle. (p. 370)

## 19.4 Varying treatment effects, interactions, and poststratification

"Once we include pre-test in the model, it is natural to interact it with the treatment effect" (p. 370).

Fit three competing models, only for grade 4.

```{r m19.4, warning = F, message = F}
m19.4 <-
  brm(data = electric %>% filter(grade == 4),
      post_test ~ treatment,
      seed = 19,
      file = "fits/m19.04")

m19.5 <-
  brm(data = electric %>% filter(grade == 4),
      post_test ~ treatment + pre_test,
      seed = 19,
      file = "fits/m19.05")

m19.6 <-
  brm(data = electric %>% filter(grade == 4),
      post_test ~ treatment + pre_test + treatment:pre_test,
      seed = 19,
      file = "fits/m19.06")
```

Check, the summaries, for each.

```{r}
print(m19.4, robust = T)
print(m19.5, robust = T)
print(m19.6, robust = T)
```

We can get a quick sense of what the interaction means with `conditional_effects()`.

```{r, fig.width = 4, fig.height = 2.75}
conditional_effects(m19.6, effects = "treatment:pre_test")
```

By default, `conditional_effects()` depicts interactions based on the mean and the mean $\pm 1$ standard deviation of the second variable in the interaction term, which is `pre_test`, in this case.

```{r}
post <- posterior_samples(m19.6) 

post %>% 
  expand(nesting(b_treatment, `b_treatment:pre_test`),
         pre_test = c(80, 120)) %>% 
  mutate(tau_hat = b_treatment + `b_treatment:pre_test` * pre_test) %>% 
  group_by(pre_test) %>% 
  median_qi(tau_hat) %>% 
  mutate_if(is.double, round, digits = 2)
```

Our estimates don't look quite like the point estimates presented in the middle of pate 317. However, if you look at the `.lower` and `.upper` columns, you'll see there's massive uncertainty in those posteriors. It might be easier to appreciate this with a coefficient plot.

```{r, fig.width = 4.5, fig.height = 1.5}
post %>% 
  expand(nesting(b_treatment, `b_treatment:pre_test`),
         pre_test = c(80, 120)) %>% 
  mutate(tau_hat = b_treatment + `b_treatment:pre_test` * pre_test) %>% 
  
  ggplot(aes(x = tau_hat, y = factor(pre_test))) +
  geom_vline(xintercept = c(4.4, 0.8),
             color = "royalblue", linetype = 2, size = 1/4) +
  stat_pointinterval(.width = c(.5, .95)) +
  labs(x = expression(hat(tau)),
       y = "pre_test")
```

We can also examine this with `conditional_effects()`, where we can specify our desired `pre_test` values with the `int_conditions` argument.

```{r, fig.width = 4, fig.height = 2.75}
conditional_effects(m19.6, 
                    effects = "treatment:pre_test",
                    int_conditions = list(pre_test = c(80, 120)))
```

We can get a further sense of the variability in $\hat \tau$ by making our version of Figure 19.7.

```{r, fig.width = 3.5, fig.height = 2}
set.seed(19)

post %>% 
  mutate(iter = 1:n()) %>% 
  slice_sample(n = 20) %>% 
  expand(nesting(b_treatment, `b_treatment:pre_test`, iter),
         pre_test = c(70, 130)) %>% 
  mutate(tau_hat = b_treatment + `b_treatment:pre_test` * pre_test) %>% 
  
  ggplot(aes(x = pre_test, y = tau_hat, group = iter)) +
  geom_line(size = 1/2, alpha = 1/2) +
  geom_hline(yintercept = 0, linetype = 2, size = 1/4, color = "grey50") +
  labs(subtitle = "treatment effect in grade 4",
       x = "pre-test",
       y = "treatment effect") +
  coord_cartesian(xlim = c(80, 120),
                  ylim = c(-5, 10))
```

We randomly selected the 20 posterior draws by way of `set.seed()` and `slice_sample()`.

If you only follow along in the text, the bit of code at the top of page 372 can be confusing. The second line includes the bit `sum(grade==4)`, which appears to be summing a subset of the object `grade`. However, none of the prior code in this chapter of the text had us making a `grade` object. This gets clarified in the `/ROS-Examples-master/ElectricCompany/electric.Rmd` file, where we find this.

```{r, eval = F}
grade <- rep(electric_wide$grade, 2)
```

A slight complication is we have been working with the `electric.csv` data, whereas this line of code is dependent on the `electric_wide.txt` data. If you do a little more legwork, you'll find out that bit of code is the same as if we had done this with our `electric` data.

```{r}
pull(electric, grade)
```

It's a vector of the `grade` values. Here's an alternative way to get what that bit of code is trying to do.

```{r}
grade <- pull(electric, grade)
sum(grade==4)
```

That second line is simply counting the number of cells in the vector that satisfy the logical criterion, `grade == 4`. Here's a more **tidyverse**-centric way of computing that value.

```{r}
electric %>% 
  summarise(sum_grade_4 = sum(grade==4))
```

Anyway, we can achieve what Gelman et al did with that block of code by working with our `post` object, which, recall, is the result of `posterior_samples()`.

```{r, message = F}
post %>% 
  # add an iteration index (i.e., a row number)
  mutate(iter = 1:n()) %>% 
  # expand the data frame to include the desired pre_test values
  expand(nesting(iter, b_treatment, `b_treatment:pre_test`),
         pre_test = filter(electric, grade == 4) %>% pull(pre_test)) %>% 
  # compute the treatment effect
  mutate(effect = b_treatment + `b_treatment:pre_test` * pre_test) %>% 
  # summarize
  group_by(iter) %>% 
  summarise(avg_effect = mean(effect)) %>% 
  summarise(median = median(avg_effect), 
            mad = mad(avg_effect))
```

The average treatment in the sample effect is about 1.8, with a mad of about 0.7, which is "similar to the result from the model adjusting for pre-test but with no interactions" (p. 372). Check that, again.

```{r}
fixef(m19.5, robust = T) %>% round(digits = 2)
```

Yep, that's close.

> In general, for a linear regression model, the estimate obtained by including the interaction, and then averaging over the data, reduces to the estimate with no interaction. The motivation for including the interaction is thus to get a better idea of how the treatment effect varies with pre-treatment predictors, not to simply estimate an average effect. (p. 372)

#### 19.4.0.1 Bonus: Fit the model 4 times to make the figure

Backing up a bit, since we only fit the interaction mofel for `grade == 4`, we were unprepared to remake Figure 19.6. To make that figure, we'll use the `purrr::map()` approach to iterate fitting the interaction model across each level of `grade`.

```{r fits3, results = "hide", message = F}
fits3 <-
  electric %>% 
  nest(data = c(X1, post_test, pre_test, treatment, supp, pair_id, gain)) %>% 
  mutate(fit = map(data, ~update(m19.6,
                                 newdata = .,
                                 seed = 19)))
```

Now our workflow will be very similar to what we used for Figure 19.5. First, we make our supplementary `abline` data.

```{r}
abline <-
  fits3 %>% 
  mutate(fixef = map(fit, ~fixef(.) %>% 
                       data.frame() %>% 
                       rownames_to_column("parameter"))) %>% 
  unnest(fixef) %>% 
  select(grade, parameter, Estimate) %>% 
  pivot_wider(names_from = parameter, values_from = Estimate) %>% 
  set_names("grade", "a", "theta", "b", "int") %>% 
  expand(nesting(grade, a, theta, b, int),
         treatment = 0:1) %>% 
  mutate(intercept = a + theta * treatment,
         slope = b + int * treatment) %>% 
  mutate(treatment = factor(treatment),
         grade     = str_c("grade ", grade))

abline
```

Second, we make Figure 19.6.

```{r, fig.width = 8, fig.height = 2.5}
electric %>% 
  mutate(treatment = factor(treatment),
         grade     = str_c("grade ", grade)) %>% 
  
  ggplot(aes(x = pre_test, y = post_test)) +
  geom_abline(data = abline,
              aes(intercept = intercept, slope = slope, linetype = treatment),
              size = 1/4) +
  geom_point(aes(fill = treatment),
             shape = 21, stroke = 1/4) +
  scale_linetype_manual(values = 2:1, breaks = NULL) +
  scale_fill_manual(values = c("black", "transparent"), breaks = NULL) +
  scale_x_continuous(expression("pre-test, "*italic(x[i])), 
                                breaks = 0:3 * 40, limits = c(0, 125)) +
  scale_y_continuous(expression("post-test, "*italic(y[i])),
                     breaks = 0:3 * 40, limits = c(0, 125)) +
  facet_wrap(~grade, nrow = 1)
```

We might also want to go beyond point estimates to get a sense of the uncertainty in the interaction effects. One way might be with a coefficient plot.

```{r, fig.width = 4, fig.height = 2}
fits3 %>% 
  mutate(post = map(fit, posterior_samples)) %>% 
  select(-data, -fit) %>% 
  unnest(post) %>% 
  mutate(grade = factor(str_c("grade ", grade),
                        levels = str_c("grade ", 4:1))) %>%
  
  ggplot(aes(x = `b_treatment:pre_test`, y = grade)) +
  geom_vline(xintercept = 0, linetype = 2, size = 1/4) +
  stat_halfeye(.width = .95, size = 1) +
  scale_x_continuous("interaction coefficient", position = "top") +
  ylab("subpopulation") +
  coord_cartesian(ylim = c(1.5, 3.8)) +
  theme(axis.ticks.y = element_blank())
```

### 19.4.1 Poststratification of conditional treatment effects gives an average treatment effect.

> In survey sampling, *stratification* refers to the procedure of dividing the population into disjoint subsets (strata), sampling separately within each stratum, and then combining the stratum samples to get a population estimate. Poststratification is the analysis of an unstratified sample, breaking the data into strata and reweighting as would have been done had the survey actually been stratified. Stratification can adjust for potential differences between sample and population using the survey design; poststratification makes such adjustments in the data analysis. (p. 372, *emphasis* in the original)

## 19.5 Challenges of interpreting regression coefficients as treatment effects

To follow along, load the `incentives.csv` data.

```{r, message = F}
incentives <- read_csv("ROS-Examples-master/Incentives/data/incentives.csv")
glimpse(incentives)
```

The columns in the data are:

* `rr_diff` is the change in response rate from the baseline condition,
* `value` is the difference in dollar value of the incentive compared to baseline,
* `prepay` indicates whether the incentive was given before the survey was conducted,
* `gift` indicates whether the incentive was in gift form (with the default condition being cash), and
* `burden` indicates whether the survey was assessed as requiring high burden of effort on respondents.

Fit the model.

```{r m19.7}
m19.7 <-
  brm(data = incentives,
   rr_diff ~ value + prepay + gift + burden,
      seed = 19,
      file = "fits/m19.07")
```

Check the summary.

```{r}
print(m19.7, robust = T)
```

Our intercept and beta parameters make up the values displayed in the table in Figure 19.8. Even though this model summary comes from values taken from a meta-analysis, Gelman et al still caution us not to interpret them as causal. The crux of their argument is:

> The resolution is that, although the incentive conditions were assigned randomly *within* each experiment, the differences in the conditions were not assigned at random *between* experiments. The difference in response rate comparing two incentive conditions within a survey is an unbiased estimate of the effect of that particular implementation of the incentive for that particular survey--but when comparing incentives implemented in different surveys, what we have is an observational study. (p. 374, *emphasis* in the original)

## 19.6 Do not adjust for post-treatment variables

"Naively adjusting for a post-treatment variable can bias the estimate of the treatment effect, *even when the treatment has been randomly assigned to study participants*" (p. 374, *emphasis* in the original).

* y as the childâ€™s IQ score measured 2 years after the treatment regime has been completed, 
* q as a continuous parenting quality measure (ranging from 0 to 1) measured one year after treatment completion, 
* z as the randomly assigned binary treatment, and 
* x as the pre-treatment measure reflecting whether both parents have a high school education (in general this could be a vector of pre-treatment predictors). 

## 19.7 Intermediate outcomes and causal paths

> Randomized experimentation is often described as a "black box" approach to causal inference. We see what goes into the box (treatments) and we see what comes out (outcomes), and we can make inferences about the relation between these inputs and outputs, without the need to see what happens *inside* the box. This section discusses some difficulties that arise from using naive techniques to try to ascertain the role of post-treatment *mediating* variables in the causal path between treatment and outcomes, as part of a well-intentioned attempt to peer inside the black box. (p. 376, *emphasis* in the original)

### 19.7.1 Hypothetical example of a binary intermediate outcome

Simply "adjusting for [a non-randomized] intermediate outcome will lead to biased estimates of the average treatment effect" (p. 376).

### 19.7.2 Regression adjusting for intermediate outcomes cannot, in general, estimate "mediating" effects.

"Some researchers who perform these analyses will say that these models are still useful... These sorts of conclusions are generally not appropriate, however, as we illustrate with a hypothetical example" (p. 377).

#### 19.7.2.1 Hypothetical scenario with direct and indirect effects.

We might simulate data based on the table displayed in Figure 19.10. Here's the first step.

```{r}
total_n <- 1000

d <-
  tibble(effected = c(0, 1, 0),
         p_c = c(0, 0, 1),
         p_t = c(0, 1, 1),
         i_c = c(60, 65, 90),
         i_t = c(70, 80, 100),
         prop = c(.1, .7, .2))

d
```

Now we'll use those proportions to expand the data set using `uncount()`. Then we'll add a `treatment` variable, which will be evenly distributed across the response types.

```{r}
# what would you like the total sample size to be?
total_n <- 1000

d <-
  d %>% 
  uncount(weights = prop * total_n) %>% 
  select(-prop) %>% 
  mutate(treatment = rep(0:1, times = n() / 2))

glimpse(d)
```

Now we'll assign observed `p` and `i` values based on `treatment`.

```{r}
d <-
  d %>% 
  mutate(i = if_else(treatment == 0, i_c, i_t),
         p = if_else(treatment == 0, p_c, p_t))

head(d)
```


Now fit the model separately based on whether the person was `effected`.

```{r m19.8}
m19.8 <-
  brm(data = d %>% filter(effected == 0),
      i ~ treatment,
      seed = 19,
      file = "fits/m19.08")

m19.9 <-
  brm(data = d %>% filter(effected == 1),
      i ~ treatment,
      seed = 19,
      file = "fits/m19.09")
```

Check the summary for each.

```{r}
print(m19.8, robust = T)
print(m19.9, robust = T)
```

Both `treatment` effects are as expected bases on the text. We have an effect of 10 for those whose parenting styles are not effected by treatment and an effect of 15 for those whose parenting styles were effected. But notice the warning message we get for the second model. This is because we fit a model on data with a purely deterministic relation between the outcome and treatment. Because there was no other source of variability in the data, the $\sigma$ posterior went to zero, which made it rough on the sampler.

#### 19.7.2.2 A regression adjusting for the intermediate outcome does not generally work.

Now try adding in `p` as a covariate.

```{r m19.10}
m19.10 <-
  brm(data = d,
      i ~ treatment + p,
      seed = 19,
      file = "fits/m19.10")
```

Check the summary.

```{r}
print(m19.10, robust = T)
```

The conditional effect on `treatment` became moderately negative!

### 19.7.3 What can theoretically be estimated: principal stratification.

Treatment effects can vary depending on the extent to which the mediating variable (in this example, parenting practices) is affected by the treatment. The key theoretical step here is to divide the population into categories based on their potential outcomes for the mediating variable--what would happen under each of the two treatment conditions. In statistical parlance, these categories are sometimes called principal strata. The problem is that the *principal strata* are generally unobserved. (p. 378, *emphasis* in the original)

### 19.7.4 Intermediate outcomes in the context of observational studies.

> If trying to adjust directly for mediating variables is problematic in the context of controlled experiments, it should come as no surprise that it generally is also problematic for observational studies. The concern is nonignorability--systematic differences between groups defined conditional on the post-treatment intermediate outcome. (p. 378)

## Session info {-}

```{r}
sessionInfo()
```

```{r, warning = F, echo = F, eval = F}
rm(list = ls())
```

```{r, echo = F, message = F, warning = F, results = "hide", eval = F}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

